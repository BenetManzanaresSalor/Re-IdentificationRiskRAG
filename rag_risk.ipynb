{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUCaGdAj9-9F"
   },
   "source": [
    "# Text Re-identification Evaluation with Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1750065415274,
     "user": {
      "displayName": "Nex Force",
      "userId": "06161749173587539613"
     },
     "user_tz": -120
    },
    "id": "eoujYMwW9-9J"
   },
   "outputs": [],
   "source": [
    "import os, csv, json\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from datetime import datetime\n",
    "from typing import Optional, List\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "\n",
    "from langchain_core.documents import Document as LangchainDocument\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "from cappr.huggingface.classify import cache_model, predict_proba_examples\n",
    "from cappr import Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG RISK k=10 chunk=128 FAISS_NoMasks_thenlper/gte-small mistralai/Mistral-7B-Instruct-v0.3_Quant=8_Prompt=5False_CAPPR_Prior=False_Cache=True_ShortIDs=False\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "ID_KEY = \"doc_id\"\n",
    "TEXT_KEY = \"text\"\n",
    "BK_KEY = \"background_knowledge\"\n",
    "USE_SHORT_IDS = False\n",
    "\n",
    "\n",
    "DATASET_NAME = \"wiki553\"\n",
    "CORPUS_FILE_PATH = \"data/wiki553/corpora/Wiki553_Corpus.json\"\n",
    "ANONYMIZATIONS_FILE_PATHS = {\n",
    "    \"St.NER3\":\"data/wiki553/anonymizations/Wiki553_St.NER3.json\",\n",
    "    \"St.NER4\":\"data/wiki553/anonymizations/Wiki553_St.NER4.json\",\n",
    "    \"St.NER7\":\"data/wiki553/anonymizations/Wiki553_St.NER7.json\",\n",
    "    \"spaCy\":\"data/wiki553/anonymizations/Wiki553_spaCy.json\",\n",
    "    \"Presidio\":\"data/wiki553/anonymizations/Wiki553_Presidio.json\",        \n",
    "    \"Word2Vec_t=0.5\":\"data/wiki553/anonymizations/Wiki553_Word2Vec_t=0.5.json\",\n",
    "    \"Word2Vec_t=0.25\":\"data/wiki553/anonymizations/Wiki553_Word2Vec_t=0.25.json\",\n",
    "    \"k-anonymity_Random\":\"data/wiki553/anonymizations/Wiki553_k-anonymity_Random.json\",\n",
    "    \"k-anonymity_Greedy\":\"data/wiki553/anonymizations/Wiki553_k-anonymity_Greedy.json\",\n",
    "    \"Manual\":\"data/wiki553/anonymizations/Wiki553_Manual.json\",\n",
    "    \"Student-LLM\":\"data/wiki553/anonymizations/Wiki553_gpt-4o-2024-09-03_Student.json\",\n",
    "    \"Sparks-LLM\":\"data/wiki553/anonymizations/Wiki553_gpt-4o-2024-09-03_Sparks.json\",        \n",
    "    \"MvM-LLM\":\"data/wiki553/anonymizations/Wiki553_gpt-4o-2024-09-03_MvM.json\",\n",
    "    \"Attributes-LLM\":\"data/wiki553/anonymizations/Wiki553_gpt-4o-2024-09-03_Attributes.json\",        \n",
    "}\n",
    "BK_FILE_PATH = \"data/wiki553/bks/Wiki553_BK=Public.json\"\n",
    "\n",
    "# Retriever\n",
    "RETRIEVER_USE_DENSE = True # Dense=FAISS | Sparse=BM25\n",
    "RETRIEVER_NAME = \"FAISS\" if RETRIEVER_USE_DENSE else \"BM25\"\n",
    "RETRIEVER_K = 10\n",
    "RETRIEVER_CHUNK_SIZE = 128\n",
    "RETRIEVER_MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]\n",
    "RETRIEVER_REMOVE_MASKING_MARKS = True\n",
    "if RETRIEVER_REMOVE_MASKING_MARKS:\n",
    "    RETRIEVER_NAME += \"_NoMasks\"\n",
    "RETRIEVER_MASKING_MARKS = [\"SENSITIVE\", \"PERSON\", \"DEM\", \"LOC\",\n",
    "                 \"ORG\", \"DATETIME\", \"QUANTITY\", \"MISC\",\n",
    "                 \"NORP\", \"FAC\", \"GPE\", \"PRODUCT\", \"EVENT\",\n",
    "                 \"WORK_OF_ART\", \"LAW\", \"LANGUAGE\", \"DATE\",\n",
    "                 \"TIME\", \"ORDINAL\", \"CARDINAL\", \"DATE_TIME\", \"DATETIME\",\n",
    "                 \"NRP\", \"LOCATION\", \"ORGANIZATION\", \"\\*\\*\\*\"]\n",
    "RETRIEVER_EMBEDDING_MODEL_NAME = \"thenlper/gte-small\" # For dense retriever\n",
    "RETRIEVER_NAME += f\"_{RETRIEVER_EMBEDDING_MODEL_NAME}\"\n",
    "\n",
    "# Reader\n",
    "READER_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\" #Alternatives: \"HuggingFaceH4/zephyr-7b-beta\" \"google/gemma-3-4b-it\" \"jet-ai/Jet-Nemotron-2B\" \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "READER_QUANTIZATION_BITS = 8\n",
    "if READER_QUANTIZATION_BITS == 4:\n",
    "    READER_QUANTIZATION_CONFIG = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "elif READER_QUANTIZATION_BITS == 8:\n",
    "    READER_QUANTIZATION_CONFIG = BitsAndBytesConfig(load_in_8bit=True)\n",
    "else:\n",
    "    READER_QUANTIZATION_CONFIG = None\n",
    "READER_BATCH_SIZE = 1\n",
    "READER_PROMPT_0_USER = [ # Only User role\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Task: Predict the ID of the person corresponding to a document.\n",
    "\n",
    "Input:\n",
    "BACKGROUND: A list of people and their details, indexed by ID.\n",
    "DOCUMENT: A text related to one person from the BACKGROUND.\n",
    "NAMES: A summary list of the IDs from people in BACKGROUND.\n",
    "\n",
    "Output:\n",
    "The ID of the person that best fits the DOCUMENT based on the BACKGROUND. Important: No reasoning, directly return the ID.\n",
    "\n",
    "----------------\n",
    "\n",
    "BACKGROUND:{background}\n",
    "\n",
    "\n",
    "DOCUMENT: {document}\n",
    "\n",
    "\n",
    "NAMES: {names}\n",
    "\n",
    "----------------\n",
    "\n",
    "Based on the input provided, the ID corresponding to the document is:\"\"\"\n",
    "    },\n",
    "]\n",
    "READER_PROMPT_1_USER_ASSISTANT = [ # User, Assistant roles\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Task: Predict the ID of the person corresponding to a document.\n",
    "\n",
    "Input:\n",
    "BACKGROUND: A list of people and their details, indexed by ID.\n",
    "DOCUMENT: A text related to one person from the BACKGROUND.\n",
    "NAMES: A summary list of the IDs from people in BACKGROUND.\n",
    "\n",
    "Output:\n",
    "The ID of the person that best fits the DOCUMENT based on the BACKGROUND. Important: No reasoning, directly return the ID.\n",
    "\n",
    "----------------\n",
    "\n",
    "BACKGROUND:{background}\n",
    "\n",
    "\n",
    "DOCUMENT: {document}\n",
    "\n",
    "\n",
    "NAMES: {names}\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\"\"Based on the input provided, the ID corresponding to the document is:\"\"\"\n",
    "    },\n",
    "]\n",
    "READER_PROMPT_2_SYSTEM_USER_ASSISTANT = [ # System, User, Assistant roles\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Task: Predict the ID of the person corresponding to a document.\n",
    "\n",
    "Input:\n",
    "BACKGROUND: A list of people and their details, indexed by ID.\n",
    "DOCUMENT: A text related to one person from the BACKGROUND.\n",
    "NAMES: A summary list of the IDs from people in BACKGROUND.\n",
    "\n",
    "Output:\n",
    "The ID of the person that best fits the DOCUMENT based on the BACKGROUND. Important: No reasoning, directly return the ID.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"BACKGROUND:{background}\n",
    "\n",
    "----------------\n",
    "\n",
    "DOCUMENT: {document}\n",
    "\n",
    "----------------\n",
    "\n",
    "NAMES: {names}\n",
    "\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\"\"Based on the input provided, the ID corresponding to the document is:\"\"\"\n",
    "    },\n",
    "]\n",
    "READER_PROMPT_3_SYSTEM_USER = [ # System, User roles\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Task: Predict the ID of the person corresponding to a document.\n",
    "\n",
    "Input:\n",
    "BACKGROUND: A list of people and their details, indexed by ID.\n",
    "DOCUMENT: A text related to one person from the BACKGROUND.\n",
    "NAMES: A summary list of the IDs from people in BACKGROUND.\n",
    "\n",
    "Output:\n",
    "The ID of the person that best fits the DOCUMENT based on the BACKGROUND. Important: No reasoning, directly return the ID.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"BACKGROUND:{background}\n",
    "\n",
    "----------------\n",
    "\n",
    "DOCUMENT: {document}\n",
    "\n",
    "----------------\n",
    "\n",
    "NAMES: {names}\n",
    "\n",
    "----------------\n",
    "\n",
    "Based on the input provided, the ID corresponding to the document is:\"\"\"\n",
    "    },\n",
    "]\n",
    "READER_PROMPT_4_SYSTEM_USER = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are an assistant that identifies which person from a given BACKGROUND corresponds to a DOCUMENT.\n",
    "\n",
    "Your task:\n",
    "- Read the BACKGROUND, which lists people and their details by ID.\n",
    "- Read the DOCUMENT, which relates to exactly one person.\n",
    "- Read NAMES, which lists all available IDs.\n",
    "- Decide which ID best matches the DOCUMENT.\n",
    "\n",
    "Output format:\n",
    "- Respond with only the matching ID, nothing else (no reasoning, no explanation).\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"BACKGROUND:\n",
    "{background}\n",
    "\n",
    "---\n",
    "\n",
    "DOCUMENT:\n",
    "{document}\n",
    "\n",
    "---\n",
    "\n",
    "NAMES:\n",
    "{names}\n",
    "\n",
    "---\n",
    "\n",
    "Return only the ID that best matches the DOCUMENT:\"\"\"\n",
    "    },\n",
    "]\n",
    "READER_PROMPT_5_USER = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Your task is to select the ID that best matches the DOCUMENT based on the BACKGROUND.\n",
    "\n",
    "Inputs:\n",
    "- BACKGROUND lists people and their details by ID.\n",
    "- DOCUMENT is related to exactly one person. It might be pseudo-anonymized.\n",
    "- IDs lists all available identifiers.\n",
    "\n",
    "Output:\n",
    "- Respond with only the matching ID. Do not include explanations or reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "BACKGROUND:\n",
    "{background}\n",
    "\n",
    "---\n",
    "\n",
    "DOCUMENT:\n",
    "{document}\n",
    "\n",
    "---\n",
    "\n",
    "IDs:\n",
    "{names}\n",
    "\n",
    "---\n",
    "\n",
    "The ID that best matches the DOCUMENT is:\"\"\"\n",
    "    },\n",
    "]\n",
    "READER_PROMPTS_LIST = [READER_PROMPT_0_USER, READER_PROMPT_1_USER_ASSISTANT,\n",
    "                        READER_PROMPT_2_SYSTEM_USER_ASSISTANT,\n",
    "                        READER_PROMPT_3_SYSTEM_USER, READER_PROMPT_4_SYSTEM_USER,\n",
    "                        READER_PROMPT_5_USER]\n",
    "READER_PROMPT_IDX = 5\n",
    "READER_SELECTED_PROMPT = READER_PROMPTS_LIST[READER_PROMPT_IDX]\n",
    "READER_PROMPT_SEPARATOR = \"----------------\" if READER_PROMPT_IDX < 4 else \"---\"\n",
    "READER_REMOVE_ROLE_END_STR = True\n",
    "READER_ROLE_END_STR = \"</s>\" if \"zephyr\" in READER_MODEL_NAME else \"[/INST]\"\n",
    "READER_USE_PRIOR_PROBABILITIES = False\n",
    "READER_USE_CAPPR_CACHE = True\n",
    "READER_NAME = f\"{READER_MODEL_NAME}_Quant={READER_QUANTIZATION_BITS}_Prompt={READER_PROMPT_IDX}{not READER_REMOVE_ROLE_END_STR}\"+ \\\n",
    "    f\"_CAPPR_Prior={READER_USE_PRIOR_PROBABILITIES}_Cache={READER_USE_CAPPR_CACHE}_ShortIDs={USE_SHORT_IDS}\"\n",
    "\n",
    "# Output\n",
    "RESULTS_FILEPATH = f\"results_{DATASET_NAME}.csv\"\n",
    "RAG_RISK_NAME = f\"RAG RISK k={RETRIEVER_K} chunk={RETRIEVER_CHUNK_SIZE} {RETRIEVER_NAME} {READER_NAME}\"\n",
    "print(RAG_RISK_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr6rN10U9-9J"
   },
   "source": [
    "# Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_text(masked_spans:list, original_text:str) -> str:\n",
    "    masked_text = \"\"+original_text\n",
    "    \n",
    "    for span in reversed(sorted(masked_spans, key=lambda x:x[0], reverse=False)):\n",
    "        start_idx = span[0]\n",
    "        end_idx = span[1]\n",
    "        if len(span)==3:\n",
    "            replacement = span[2]\n",
    "        else: # If there is no replacement, use first masking mark\n",
    "            replacement = RETRIEVER_MASKING_MARKS[0]\n",
    "        masked_text = masked_text[:start_idx] + replacement + masked_text[end_idx:]\n",
    "    \n",
    "    return masked_text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "# Load corpus (ids and texts)\n",
    "with open(CORPUS_FILE_PATH, \"r\") as f:\n",
    "    corpus = json.load(f)\n",
    "for retrieved in corpus:\n",
    "    id = retrieved[ID_KEY]\n",
    "    data[id] = {ID_KEY:id, TEXT_KEY:retrieved[TEXT_KEY]}\n",
    "del corpus\n",
    "\n",
    "# Load background knowledge\n",
    "with open(BK_FILE_PATH, \"r\") as f:\n",
    "    bk = json.load(f)\n",
    "for id, bk_text in bk.items():\n",
    "    data[id][BK_KEY] = bk_text\n",
    "\n",
    "# Load anonymizations\n",
    "for anon_name, anon_file_path in ANONYMIZATIONS_FILE_PATHS.items():\n",
    "    with open(anon_file_path, \"r\") as f:\n",
    "        anon = json.load(f)\n",
    "    for id, masked_spans in anon.items():\n",
    "        data[id][anon_name] = get_masked_text(masked_spans, data[id][TEXT_KEY])\n",
    "\n",
    "if USE_SHORT_IDS:\n",
    "    # Tranform IDs (full names) to short codes\n",
    "    new_data = {}\n",
    "    new_ids = {chr(65+i//26)+chr(65+(i%26)) for i in range(len(data))}\n",
    "    for new_id, value in zip(new_ids, data.values()):\n",
    "        value[ID_KEY] = new_id\n",
    "        new_data[new_id] = value\n",
    "    data = new_data\n",
    "\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1750063993638,
     "user": {
      "displayName": "Nex Force",
      "userId": "06161749173587539613"
     },
     "user_tz": -120
    },
    "id": "__bIMWbS6qpY"
   },
   "outputs": [],
   "source": [
    "# Load from dataframe\n",
    "df = pd.DataFrame.from_dict(data.values())\n",
    "\n",
    "ds = []\n",
    "for idx, row in df.iterrows():\n",
    "  if row[BK_KEY] is not None and row[BK_KEY].strip() != \"\":\n",
    "    retrieved = {\"text\": row[BK_KEY], \"source\": row[ID_KEY]}\n",
    "    ds.append(retrieved)\n",
    "\n",
    "#print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ff4f666c794fba90bc0b5edfc82f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert into LangchainDocuments\n",
    "raw_knowledge_base = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(ds)\n",
    "]\n",
    "\n",
    "#print(len(raw_knowledge_base))\n",
    "#print(raw_knowledge_base[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_LxjD5h9-9K"
   },
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0869742543f8445098e77cd0bfc7d8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: Optional[str] = RETRIEVER_EMBEDDING_MODEL_NAME,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=RETRIEVER_MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in tqdm(knowledge_base):\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique\n",
    "\n",
    "docs_processed = split_documents(\n",
    "    RETRIEVER_CHUNK_SIZE,\n",
    "    raw_knowledge_base,\n",
    "    tokenizer_name=RETRIEVER_EMBEDDING_MODEL_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uS6Mv8O9-9L"
   },
   "source": [
    "## Instanciate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "0661aae5d1f745f0bfee4e6cb81b1ad5",
      "25dfb9d774a14fad83cdbdf5f1a6a8aa",
      "2977382c1e8748199aba1e8e0efccce7",
      "2e20f5f2a06142ccbff809080b8ed312",
      "f991e7944b6348e1929e2038ca9133e0",
      "6e5484574e7345ee80fba8e2e71e8c60",
      "e332f5163c8c4458848896a5fac6a2b9",
      "e50d3cff4960410facbc6fa54b438f43",
      "b68b3f775f8245d388abceeefa04dcd5",
      "4c307e5f5bc94054a2eaf68a6e5b56cb",
      "4fc74dc4fab0486783934bdb691c648b"
     ]
    },
    "executionInfo": {
     "elapsed": 27134,
     "status": "ok",
     "timestamp": 1750064030428,
     "user": {
      "displayName": "Nex Force",
      "userId": "06161749173587539613"
     },
     "user_tz": -120
    },
    "id": "9hvIL2jO9-9M",
    "outputId": "9cfeb92d-5e08-4e54-c9ac-15e19ee8b469"
   },
   "outputs": [],
   "source": [
    "if RETRIEVER_USE_DENSE:\n",
    "    # Embedding model in GPU\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=RETRIEVER_EMBEDDING_MODEL_NAME,\n",
    "        model_kwargs={\n",
    "            \"device\": \"cuda\"\n",
    "        },\n",
    "        encode_kwargs={\n",
    "            \"normalize_embeddings\": True, # Set `True` for cosine similarity\n",
    "            \"batch_size\": 32,  # Process embeddings in batches\n",
    "        },\n",
    "        multi_process=False\n",
    "    )\n",
    "\n",
    "    # Create FAISS index\n",
    "    retriever = FAISS.from_documents(\n",
    "        docs_processed,\n",
    "        embedding_model,\n",
    "        distance_strategy=DistanceStrategy.COSINE\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # Create BM25 retriever\n",
    "    retriever = BM25Retriever.from_documents(docs_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_df(df, retriever, k)->dict:\n",
    "  retrievals = {}\n",
    "\n",
    "  for col_name in df.columns:\n",
    "      if col_name in [BK_KEY, ID_KEY]: # Exclude unnecesary columns\n",
    "          continue\n",
    "      retrievals[col_name] = retrieval_col(df, col_name, retriever, k)\n",
    "\n",
    "  return retrievals\n",
    "\n",
    "def retrieval_col(df, col_name, retriever, k)->list:\n",
    "  retrievals = []\n",
    "  with tqdm(total=len(df), desc=f\"Retrievals for {col_name=}\") as pbar:\n",
    "      for idx, row in df.iterrows():\n",
    "          text = row[col_name]\n",
    "          if text is None or text.strip() == \"\":\n",
    "            retrievals.append(None)\n",
    "            continue\n",
    "\n",
    "          retrieved_docs = retrieval_doc(text, retriever, k)\n",
    "          retrievals.append(retrieved_docs)\n",
    "          pbar.update(1)\n",
    "\n",
    "  return retrievals\n",
    "\n",
    "def retrieval_doc(text, retriever, k, use_masking_marks_removal:bool=RETRIEVER_REMOVE_MASKING_MARKS):\n",
    "    if use_masking_marks_removal:\n",
    "        text = remove_masking_marks(text)\n",
    "    if type(retriever)==BM25Retriever:\n",
    "        retriever.k = k # Force the proper k\n",
    "        result = retriever.invoke(text)\n",
    "    else: # FAISS\n",
    "        result = retriever.similarity_search(query=text, k=k)\n",
    "    return result\n",
    "\n",
    "def remove_masking_marks(original_text:str, masking_marks:list=RETRIEVER_MASKING_MARKS):\n",
    "    new_text = original_text\n",
    "    for mark in masking_marks:\n",
    "        new_text = new_text.replace(mark, \"\").strip()\n",
    "        new_text = ' '.join(new_text.split())\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17b2107ad3044c4b23133b8b7c731db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    READER_MODEL_NAME,\n",
    "    config=AutoConfig.from_pretrained(READER_MODEL_NAME),\n",
    "    quantization_config=READER_QUANTIZATION_CONFIG,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "original_forward = model.forward\n",
    "def cached_forward(*args, **kwargs):\n",
    "    kwargs[\"use_cache\"] = True\n",
    "    return original_forward(*args, **kwargs)\n",
    "model.forward = cached_forward\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "model_and_tokenizer = (model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Your task is to select the ID that best matches the DOCUMENT based on the BACKGROUND.\n",
      "\n",
      "Inputs:\n",
      "- BACKGROUND lists people and their details by ID.\n",
      "- DOCUMENT is related to exactly one person. It might be pseudo-anonymized.\n",
      "- IDs lists all available identifiers.\n",
      "\n",
      "Output:\n",
      "- Respond with only the matching ID. Do not include explanations or reasoning.\n",
      "\n",
      "---\n",
      "\n",
      "BACKGROUND:\n",
      "{background}\n",
      "\n",
      "---\n",
      "\n",
      "DOCUMENT:\n",
      "{document}\n",
      "\n",
      "---\n",
      "\n",
      "IDs:\n",
      "{names}\n",
      "\n",
      "---\n",
      "\n",
      "The ID that best matches the DOCUMENT is:[/INST]\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "rag_prompt_template = tokenizer.apply_chat_template(\n",
    "    READER_SELECTED_PROMPT, tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(rag_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87264718 0.12735282]\n",
      "[8.05619281e-06 9.83392846e-01 1.65990978e-02]\n",
      "[8.74034594e-01 1.04346800e-01 2.14904272e-02 1.28178625e-04]\n"
     ]
    }
   ],
   "source": [
    "# Testing it does not return NaN\n",
    "examples = [\n",
    "    Example(\n",
    "        prompt=\"Jodie Foster played\",\n",
    "        completions=[\"Clarice Starling\", \"Trinity in The Matrix\"],\n",
    "    ),\n",
    "    Example(\n",
    "        prompt=\"Batman, from Batman: The Animated Series, was played by\",\n",
    "        completions=(\"Pete Holmes\", \"Kevin Conroy\", \"Spongebob!\"),\n",
    "    ),\n",
    "    Example(\n",
    "        prompt=\"Scott Andrew Caan (born August 23, 1976) is an American actor. He currently stars as Detective Danny \\\"Danno\\\" Williams in the CBS television series Hawaii Five-0 (2010–present), for which he was nominated for a Golden Globe Award. Caan is also known for his recurring role as manager Scott Lavin in the HBO television series Entourage (2009–2011). He was also a part of 1990s rap group The Whooliganz with The Alchemist. The duo went by the names Mad Skillz and Mudfoot, respectively.\",\n",
    "        completions=['Scott Caan', 'Dwayne Johnson', 'Tom Hanks', 'Patrick Stewart'],\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Run CAPPR\n",
    "pred_probs = predict_proba_examples(\n",
    "    examples, model_and_tokenizer=(model, tokenizer), batch_size=READER_BATCH_SIZE\n",
    ")\n",
    "for pred in pred_probs:\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-identification risk assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_linkage_df(df:pd.DataFrame, retrievals:dict, id_to_label:dict, prompt_template) -> dict:\n",
    "  predictions = {}\n",
    "  model_and_tokenizer = (model, tokenizer)\n",
    "\n",
    "  # For each column\n",
    "  for col_name, col_retrievals in retrievals.items():\n",
    "    col_documents = df[col_name]\n",
    "    col_probs = rag_linkage_col(col_documents, col_retrievals, model_and_tokenizer, prompt_template, id_to_label)\n",
    "    predictions[col_name] = col_probs\n",
    "\n",
    "  return predictions\n",
    "\n",
    "def rag_linkage_col(col_documents, col_retrievals, model_and_tokenizer, prompt_template, id_to_label) -> np.ndarray:\n",
    "    col_probs = np.zeros((len(col_documents), len(id_to_label)))\n",
    "    model_needs_caching = READER_USE_CAPPR_CACHE # Only for first caching\n",
    "    \n",
    "    # Generate all the prompts for pipeline batching\n",
    "    examples = []\n",
    "    example_idx_to_doc_idx = {}\n",
    "    example_idx = 0\n",
    "    for doc_idx, (document, doc_retrievals) in enumerate(zip(col_documents, col_retrievals)):\n",
    "        if not doc_retrievals is None: # If none, argmax will do the equivalent to random guess\n",
    "            prompt, retrieved_ids, retrieved_ids_counts = rag_linkage_construct_prompt(doc_retrievals, document, prompt_template)\n",
    "\n",
    "            # If only one individual retrieved, that is the response\n",
    "            if len(retrieved_ids) == 1:\n",
    "                label = id_to_label[retrieved_ids[0]]\n",
    "                col_probs[doc_idx][label] = 1\n",
    "            # Otherwise, reader prediction required\n",
    "            else:\n",
    "                retrieved_labels = [id_to_label[id] for id in retrieved_ids]\n",
    "\n",
    "                # Computing prior probabilities\n",
    "                if READER_USE_PRIOR_PROBABILITIES:                    \n",
    "                    for id, count in retrieved_ids_counts.items():\n",
    "                        col_probs[doc_idx][id_to_label[id]] += count\n",
    "                    col_probs[doc_idx][retrieved_labels] = np.exp(col_probs[doc_idx][retrieved_labels]) / np.sum(np.exp(col_probs[doc_idx][retrieved_labels]))\n",
    "                    prior = col_probs[doc_idx][retrieved_labels]\n",
    "                else:\n",
    "                    prior = None\n",
    "                \n",
    "                if READER_USE_CAPPR_CACHE:\n",
    "                    # Cache model for first time\n",
    "                    if model_needs_caching:\n",
    "                        prompt_prefix = prompt.split(READER_PROMPT_SEPARATOR)[0]+READER_PROMPT_SEPARATOR\n",
    "                        model_and_tokenizer = cache_model(\n",
    "                            model_and_tokenizer, prompt_prefix\n",
    "                        )\n",
    "                        model_needs_caching = False\n",
    "                    # The rest of the text is the prompt                    \n",
    "                    prompt = prompt = prompt[len(prompt_prefix):]\n",
    "\n",
    "                # Create exmaple for CAPPR\n",
    "                examples.append(Example(prompt=prompt,\n",
    "                        completions=retrieved_ids,\n",
    "                        prior=prior))\n",
    "                example_idx_to_doc_idx[example_idx] = doc_idx\n",
    "                example_idx += 1\n",
    "    \n",
    "    # Performing CAPPR predictions\n",
    "    rag_linkage_cappr_predict(col_probs, model_and_tokenizer, examples, example_idx_to_doc_idx, id_to_label)\n",
    "\n",
    "    return col_probs\n",
    "\n",
    "def rag_linkage_construct_prompt(doc_retrievals, document, prompt_template):\n",
    "    # Obtain retrievals grouped by id\n",
    "    doc_retrievals_dict = {}\n",
    "    retrieved_ids_counts = {}\n",
    "    for retrieved in doc_retrievals:\n",
    "       id = retrieved.metadata[\"source\"]\n",
    "       content = retrieved.page_content\n",
    "       doc_retrievals_dict[id] = doc_retrievals_dict.get(id, \"\") + f\"\\t{content}\\n\"\n",
    "       retrieved_ids_counts[id] = retrieved_ids_counts.get(id, 0) + 1\n",
    "    \n",
    "    # Generate list of retrieved names\n",
    "    retrieved_names = list(doc_retrievals_dict.keys())\n",
    "\n",
    "    # Generate background\n",
    "    background = \"\".join(\n",
    "        [f\"\\nID={id}\\n{text}\" for id, text in doc_retrievals_dict.items()]\n",
    "    )\n",
    "\n",
    "    # Generate the prompt\n",
    "    prompt = prompt_template.format(\n",
    "        document=document, background=background, names=retrieved_names\n",
    "    )\n",
    "    \n",
    "    if READER_REMOVE_ROLE_END_STR:\n",
    "        prompt = prompt[:-(len(READER_ROLE_END_STR)+1)] # Remove the end role/sequence mark\n",
    "\n",
    "    return prompt, retrieved_names, retrieved_ids_counts\n",
    "\n",
    "def rag_linkage_cappr_predict(col_probs, model_and_tokenizer, examples, example_idx_to_doc_idx, id_to_label):\n",
    "    pred_probs = predict_proba_examples(examples, model_and_tokenizer=model_and_tokenizer, batch_size=READER_BATCH_SIZE)\n",
    "    for example_idx, (probs, example) in enumerate(zip(pred_probs, examples)):\n",
    "        doc_idx = example_idx_to_doc_idx[example_idx]\n",
    "        retrieved_names = example.completions\n",
    "        retrieved_labels = [id_to_label[name] for name in retrieved_names]\n",
    "        col_probs[doc_idx][retrieved_labels] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rag_linkage_df(df:pd.DataFrame, k:int, id_to_label:dict, retrievals:dict, model_and_tokenizer, prompt_template, verbose:bool=True):\n",
    "    predictions = {}\n",
    "    top_1_accuracies = {}\n",
    "    top_k_accuracies = {}\n",
    "\n",
    "    for col_name, col_retrievals in retrievals.items():\n",
    "        if verbose:\n",
    "            print(f\"Evaluation of {col_name} documents\")\n",
    "        \n",
    "        predictions[col_name] = rag_linkage_col(df[col_name], col_retrievals, model_and_tokenizer, prompt_template, id_to_label)\n",
    "        top_1_accuracies[col_name], top_k_accuracies[col_name] = eval_linkage_col(df, predictions[col_name], k, id_to_label)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Top-1 accuracies for {col_name}: {top_1_accuracies[col_name]}\")\n",
    "            print(f\"Top-{k} accuracies for {col_name}: {top_k_accuracies[col_name]}\")\n",
    "    \n",
    "    return top_1_accuracies, top_k_accuracies, predictions\n",
    "\n",
    "def eval_linkage_df(df:pd.DataFrame, predictions:dict, k:int, id_to_label:dict)->dict:\n",
    "  top_1_accuracies = {}\n",
    "  top_k_accuracies = {}\n",
    "\n",
    "  for col_name, preds in predictions.items():\n",
    "    top_1_accuracies[col_name], top_k_accuracies[col_name] = eval_linkage_col(df, preds, k, id_to_label)\n",
    "\n",
    "  return top_1_accuracies, top_k_accuracies   \n",
    "\n",
    "def eval_linkage_col(df, preds:np.ndarray, k:int, id_to_label:dict)->tuple:\n",
    "  top_1_count = 0\n",
    "  top_k_count = 0\n",
    "\n",
    "  for idx, row in df.iterrows():\n",
    "    # If there is a prediction\n",
    "    if preds[idx].sum() != 0:\n",
    "      id = row[ID_KEY]\n",
    "      label = id_to_label[id]\n",
    "      top_1_count += 1 if np.argmax(preds[idx]) == label else 0\n",
    "      top_k_count += 1 if label in np.argsort(preds[idx])[-k:] else 0\n",
    "\n",
    "  top_1_accuracy = 100 * top_1_count / len(df)\n",
    "  top_k_accuracy = 100 * top_k_count / len(df)\n",
    "\n",
    "  return top_1_accuracy, top_k_accuracy\n",
    "\n",
    "def accuracies_to_csv(accuracy_data: dict, method_name: str, filename: str):\n",
    "    try:\n",
    "        # Get the names of the datasets from the dictionary keys.\n",
    "        # These will serve as the main column headers for the accuracy values.\n",
    "        dataset_names = list(accuracy_data.keys())\n",
    "\n",
    "        # Construct the header row for the CSV.\n",
    "        # The first element is a label for the method name column, followed by dataset names.\n",
    "        header_row = ['Method'] + dataset_names\n",
    "\n",
    "        # Construct the data row for the method's accuracies.\n",
    "        # It starts with the method's name, then appends each accuracy score\n",
    "        # corresponding to the order of `dataset_names`.\n",
    "        datetime_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "        data_row = [datetime_str, method_name]\n",
    "        for dataset in dataset_names:\n",
    "            # Ensure the accuracy value exists for the dataset\n",
    "            if dataset in accuracy_data:\n",
    "                data_row.append(f\"{accuracy_data[dataset]:.2f}\")\n",
    "            else:\n",
    "                # Append an empty string or a placeholder if a dataset is missing\n",
    "                # (though `dataset_names` is derived from `accuracy_data` keys,\n",
    "                # this provides robustness if data structures change).\n",
    "                data_row.append('')\n",
    "\n",
    "        # Determine if the file exists to decide whether to write headers and append or overwrite.\n",
    "        file_exists = os.path.exists(filename)\n",
    "\n",
    "        # Open the CSV file. Use 'w' mode if it's a new file (to write headers),\n",
    "        # otherwise use 'a' mode (append) if it already exists.\n",
    "        # `newline=''` is crucial for CSV files to prevent extra blank rows.\n",
    "        with open(filename, 'a+', newline='', encoding='utf-8') as csvfile:\n",
    "            # Create a CSV writer object.\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "\n",
    "            # Write the header row ONLY if the file did not exist previously.\n",
    "            if not file_exists:\n",
    "                csv_writer.writerow(header_row)\n",
    "\n",
    "            # Write the data row to the CSV\n",
    "            csv_writer.writerow(data_row)\n",
    "\n",
    "        print(f\"✅ Accuracy data for method '{method_name}' successfully written to '{filename}'.\")\n",
    "\n",
    "    except IOError as e:\n",
    "        print(f\"❌ Error writing to file '{filename}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04fc88b9540430da6b0722d14f7c734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrievals for col_name='text':   0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad46a4a719cb49708f9f564df834d51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrievals for col_name='St.NER3':   0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f933c27eb8549f38ce6b1e380f6705a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrievals for col_name='St.NER4':   0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072622bcc8154d7b9955e42645f5c79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrievals for col_name='St.NER7':   0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d806a6f00640158c227973e2aad923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrievals for col_name='spaCy':   0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4175081261c44d389ffc1b8f383d5d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrievals for col_name='Presidio':   0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bf414e37854acfb8d8345a855870d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrievals for col_name='Word2Vec_t=0.5':   0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16403a994f6b4de89897aa1cbf5b0f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrievals for col_name='Word2Vec_t=0.25':   0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ecbd90cac624faa83196e9a4e40d05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrievals for col_name='k-anonymity_Random':   0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765b0f37d62242fb9efecbd34838a86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrievals for col_name='k-anonymity_Greedy':   0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fbb106458048419445fe324c1a1f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrievals for col_name='Manual':   0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b11eec4f99843d3bbecfaf16e989776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrievals for col_name='Student-LLM':   0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6379127cd84ef5b4f02f79ae084b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrievals for col_name='Sparks-LLM':   0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d04e21bf7243c7864b223aa7cc4df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrievals for col_name='MvM-LLM':   0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53384111bf7b489381da2f7642f7298a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrievals for col_name='Attributes-LLM':   0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform all retrievals\n",
    "id_to_label = {id:idx for idx, id in enumerate(df[ID_KEY])}\n",
    "retrievals = retrieval_df(df, retriever, RETRIEVER_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_top_1_accuracies, rag_top_k_accuracies, rag_predictions = eval_rag_linkage_df(df, RETRIEVER_K, id_to_label, retrievals, model_and_tokenizer, rag_prompt_template)\n",
    "accuracies_to_csv(rag_top_1_accuracies, RAG_RISK_NAME, RESULTS_FILEPATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "D4jkRckY6hK6",
    "pQk_CJEa6nwB"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "RAG_TRIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
